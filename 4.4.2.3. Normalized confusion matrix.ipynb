{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a254f",
   "metadata": {},
   "source": [
    "# 4.4.2.3. Normalized confusion matrix\n",
    "\n",
    "Effects of the normalization of the confusion matrix on the uncertainty of the results in strongly imbalanced datasets.\n",
    "\n",
    "Starting from the confusion matrices of the four teams of the MoNuSAC competitions for which the predictions are available (as computed in [Foucart, 2022](http://dx.doi.org/10.13140/RG.2.2.11627.00801))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e1d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc708bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification CMs\n",
      "Team 1\n",
      "[[6098  260    8   12]\n",
      " [  79 7214    2    1]\n",
      " [   5   39  118    2]\n",
      " [  16   11    8  170]]\n",
      "Team 2\n",
      "[[6240   88    0   57]\n",
      " [ 162 7131    4    6]\n",
      " [   1   18  133   10]\n",
      " [  16    1   11  164]]\n",
      "Team 3\n",
      "[[5960   96    0    1]\n",
      " [  76 6864    3    0]\n",
      " [   1   20  137    3]\n",
      " [  30    8   11  159]]\n",
      "Team 4\n",
      "[[6193  302    2   22]\n",
      " [ 179 7274    1    0]\n",
      " [   3   38  117    2]\n",
      " [  30    7   25  155]]\n",
      "Detection FPs/FNs/TPs\n",
      "Team 1\n",
      "FPs=[1338  829   14   59]\n",
      "FNs=[831 507   8 102]\n",
      "TPs=[6378 7296  164  205]\n",
      "Team 2\n",
      "FPs=[962 629   5 285]\n",
      "FNs=[824 500  10 115]\n",
      "TPs=[6385 7303  162  192]\n",
      "Team 3\n",
      "FPs=[2932 1545   50   99]\n",
      "FNs=[1152  860   11   99]\n",
      "TPs=[6057 6943  161  208]\n",
      "Team 4\n",
      "FPs=[1035  770   10   13]\n",
      "FNs=[690 349  12  90]\n",
      "TPs=[6519 7454  160  217]\n"
     ]
    }
   ],
   "source": [
    "CLASS_LABELS = [\"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"]\n",
    "\n",
    "CMs = np.array([\n",
    "    [\n",
    "        [0, 1338, 829, 14, 59],\n",
    "        [831, 6098, 260, 8, 12],\n",
    "        [507, 79, 7214, 2, 1],\n",
    "        [8, 5, 39, 118, 2],\n",
    "        [102, 16, 11, 8, 170]\n",
    "    ],\n",
    "    [\n",
    "        [0, 962, 629, 5, 285],\n",
    "        [824, 6240, 88, 0, 57],\n",
    "        [500, 162, 7131, 4, 6],\n",
    "        [10, 1, 18, 133, 10],\n",
    "        [115, 16, 1, 11, 164]\n",
    "    ],\n",
    "    [\n",
    "        [0, 2932, 1545, 50, 99],\n",
    "        [1152, 5960, 96, 0, 1],\n",
    "        [860, 76, 6864, 3, 0],\n",
    "        [11, 1, 20, 137, 3],\n",
    "        [99, 30, 8, 11, 159]\n",
    "    ],\n",
    "    [\n",
    "        [0, 1035, 770, 10, 13],\n",
    "        [690, 6193, 302, 2, 22],\n",
    "        [349, 179, 7274, 1, 0],\n",
    "        [12, 3, 38, 117, 2],\n",
    "        [90, 30, 7, 25, 155]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"Classification CMs\")\n",
    "for i, cm in enumerate(CMs):\n",
    "    print(f\"Team {i+1}\")\n",
    "    print(cm[1:, 1:])\n",
    "    \n",
    "print(\"Detection FPs/FNs/TPs\")\n",
    "for i, cm in enumerate(CMs):\n",
    "    print(f\"Team {i+1}\")\n",
    "    print(f\"FPs={cm[0, 1:]}\")\n",
    "    print(f\"FNs={cm[1:, 0]}\")\n",
    "    print(f\"TPs={cm[1:, 1:].sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcfa20",
   "metadata": {},
   "source": [
    "The following values are computed:\n",
    "\n",
    "* $\\pi_c$ : the class proportions based on the teams' predictions\n",
    "* Detection $REC_c$: the RECALL values for each class for the *detection* problem (i.e. an object that is detected is counted regardless of its predicted class)\n",
    "* Classification $SEN_c$: the SENSITIVITY values for each class for the *classification* problem (i.e. only counting objects which were correctly detected)\n",
    "\n",
    "We also compute the \"error distribution matrix\" of the classification task (i.e. if team makes a mistake on a class C sample, where will it be classified?), for the simulation in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd2f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per class in the ground truth:\n",
      "Epithelial : 7209\n",
      "Lymphocyte : 7803\n",
      "Neutrophil : 172\n",
      "Macrophage : 307\n",
      "Proportion of samples per class in the ground truth:\n",
      "Epithelial : 0.47\n",
      "Lymphocyte : 0.50\n",
      "Neutrophil : 0.01\n",
      "Macrophage : 0.02\n",
      "Predicted PI_C\n",
      "Epithelial: [0.46, 0.50]\n",
      "Lymphocyte: [0.47, 0.52]\n",
      "Neutrophil: [0.01, 0.01]\n",
      "Macrophage: [0.01, 0.03]\n",
      "Detection REC_C\n",
      "Epithelial: [0.84, 0.90]\n",
      "Lymphocyte: [0.89, 0.96]\n",
      "Neutrophil: [0.93, 0.95]\n",
      "Macrophage: [0.63, 0.71]\n",
      "Classification SEN_C\n",
      "Epithelial: [0.95, 0.98]\n",
      "Lymphocyte: [0.98, 0.99]\n",
      "Neutrophil: [0.72, 0.85]\n",
      "Macrophage: [0.71, 0.85]\n"
     ]
    }
   ],
   "source": [
    "Ns = CMs[0, 1:].sum(axis=1)\n",
    "pis = Ns/Ns.sum()\n",
    "\n",
    "print(\"Number of samples per class in the ground truth:\")\n",
    "for label, n in zip(CLASS_LABELS, Ns):\n",
    "    print(f\"{label} : {n}\")\n",
    "print(\"Proportion of samples per class in the ground truth:\")\n",
    "for label, pic in zip(CLASS_LABELS, pis):\n",
    "    print(f\"{label} : {pic:.2f}\")\n",
    "\n",
    "rec_c_per_team = []\n",
    "sen_c_per_team = []\n",
    "fpr_per_team = []\n",
    "pis_per_team = []\n",
    "error_distrib_per_team = []\n",
    "\n",
    "for cm in CMs:\n",
    "    rec_cs = np.array([row[1:].sum()/row.sum() for row in cm[1:]])\n",
    "    rec_c_per_team.append(rec_cs)\n",
    "    \n",
    "    sen_c_per_team.append(np.diagonal(cm[1:,1:])/cm[1:,1:].sum(axis=1))\n",
    "    \n",
    "    team_pis = cm[:, 1:].sum(axis=0)\n",
    "    pis_per_team.append(team_pis/team_pis.sum())\n",
    "    \n",
    "    team_fpr = cm[0, 1:]/Ns.sum()\n",
    "    fpr_per_team.append(team_fpr)\n",
    "    team_error_distrib = np.array([row/row.sum() for row in cm[1:,1:]])\n",
    "    error_distrib_per_team.append(team_error_distrib)\n",
    "\n",
    "print(\"Predicted PI_C\")\n",
    "for idl, label in enumerate(CLASS_LABELS):\n",
    "    pis_teams = np.array([pis_per_team[idt][idl] for idt in range(4)])\n",
    "    print(f\"{label}: [{pis_teams.min():.2f}, {pis_teams.max():.2f}]\")\n",
    "\n",
    "print(\"Detection REC_C\")\n",
    "for idl, label in enumerate(CLASS_LABELS):\n",
    "    rec_teams = np.array([rec_c_per_team[idt][idl] for idt in range(4)])\n",
    "    print(f\"{label}: [{rec_teams.min():.2f}, {rec_teams.max():.2f}]\")\n",
    "    \n",
    "print(\"Classification SEN_C\")\n",
    "for idl, label in enumerate(CLASS_LABELS):\n",
    "    sen_teams = np.array([sen_c_per_team[idt][idl] for idt in range(4)])\n",
    "    print(f\"{label}: [{sen_teams.min():.2f}, {sen_teams.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e0590",
   "metadata": {},
   "source": [
    "Computing all the classification metrics (looking only at detected samples, so it's a \"4 class\" problem, not a \"4 class + background\" problem), based on the CM and the NCM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2deac551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team    Acc    GM     MCCn   hF1    sF1    kUn  ||  Acc    GM     MCCn   hF1    sF1    kUn \n",
      "Team 1  0.968  0.867  0.970  0.902  0.972  0.970  0.873  0.867  0.919  0.883  0.887  0.916\n",
      "Team 2  0.973  0.904  0.975  0.897  0.978  0.975  0.907  0.904  0.939  0.908  0.931  0.938\n",
      "Team 3  0.981  0.892  0.982  0.928  0.984  0.982  0.897  0.892  0.933  0.901  0.915  0.931\n",
      "Team 4  0.957  0.834  0.959  0.870  0.962  0.959  0.843  0.834  0.899  0.851  0.872  0.895\n"
     ]
    }
   ],
   "source": [
    "from metrics.classification import GM, MCC, Accuracy, kappaU, hF1, sF1\n",
    "from metrics.unbalance import rescale_metric\n",
    "from functools import partial\n",
    "\n",
    "metrics = {\n",
    "    'Acc ': Accuracy, \n",
    "    'GM  ': GM,\n",
    "    'MCCn': partial(rescale_metric, MCC), \n",
    "    'hF1 ': hF1, \n",
    "    'sF1 ': sF1, \n",
    "    'kUn ': partial(rescale_metric, kappaU)\n",
    "}\n",
    "\n",
    "res_per_team = np.zeros((4,len(metrics)*2))\n",
    "\n",
    "#print(\"CM\")\n",
    "for idt in range(4):\n",
    "    #print(f'Team {idt+1}')\n",
    "    for idm,(key,metric) in enumerate(metrics.items()):\n",
    "        res = metric(CMs[idt,1:,1:])\n",
    "        res_per_team[idt][idm] = res\n",
    "        #print(f'{key}:\\t {res:.3f}')\n",
    "        \n",
    "#print(\"NCM\")\n",
    "for idt in range(4):\n",
    "    #print(f'Team {idt+1}')\n",
    "    NCM = CMs[idt, 1:, 1:]\n",
    "    NCM = np.array([row/row.sum() for row in NCM])\n",
    "    for idm,(key,metric) in enumerate(metrics.items()):\n",
    "        res = metric(NCM)\n",
    "        res_per_team[idt][len(metrics)+idm] = res\n",
    "        #print(f'{key}:\\t {metric(NCM):.3f}')\n",
    "        \n",
    "print(f\"Team    {'   '.join(list(metrics.keys()))} ||  {'   '.join(list(metrics.keys()))}\")\n",
    "for idt in range(4):\n",
    "    res = [f'{r:.3f}' for r in res_per_team[idt]]\n",
    "    print(f\"Team {idt+1}  {'  '.join(res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa79dfe",
   "metadata": {},
   "source": [
    "**Simulation results**\n",
    "\n",
    "* Create \"fake\" dataset of N randomly sampled examples based on the MoNuSAC distribution.\n",
    "* Recompute the \"confusion matrices\" using the recall and sensitivities\n",
    "* Recompute all the metrics\n",
    "* Repeat 1.000 times and look at how much the metrics can change due to random sampling\n",
    "\n",
    "The reported results are the \"uncertainty\" due to the random sampling, computed as the maximum divergence across the four teams, with the divergence the percentile 97.5 - percentile 2.5 of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096d3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling based on dataset pis:\n",
    "def get_sample(N):\n",
    "    samples = []\n",
    "    draw = np.random.random((N,))\n",
    "    classes = np.zeros_like(draw)\n",
    "    cumul = 0\n",
    "    for c,pi in enumerate(pis):\n",
    "        selected = draw <= pi+cumul\n",
    "        draw[selected] = 1.5\n",
    "        classes[selected] = c\n",
    "        cumul += pi\n",
    "    \n",
    "    return np.array([(classes==c).sum() for c in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29834d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:24<00:00, 204.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : CM = 0.001, NCM = 0.003\n",
      "GM  : CM = 0.003, NCM = 0.003\n",
      "MCCn: CM = 0.001, NCM = 0.002\n",
      "hF1 : CM = 0.006, NCM = 0.003\n",
      "sF1 : CM = 0.001, NCM = 0.002\n",
      "kUn : CM = 0.001, NCM = 0.002\n",
      "5000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:21<00:00, 229.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : CM = 0.002, NCM = 0.010\n",
      "GM  : CM = 0.011, NCM = 0.011\n",
      "MCCn: CM = 0.002, NCM = 0.006\n",
      "hF1 : CM = 0.011, NCM = 0.009\n",
      "sF1 : CM = 0.001, NCM = 0.007\n",
      "kUn : CM = 0.002, NCM = 0.006\n",
      "1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:19<00:00, 253.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : CM = 0.006, NCM = 0.052\n",
      "GM  : CM = 0.056, NCM = 0.056\n",
      "MCCn: CM = 0.005, NCM = 0.035\n",
      "hF1 : CM = 0.046, NCM = 0.054\n",
      "sF1 : CM = 0.003, NCM = 0.035\n",
      "kUn : CM = 0.005, NCM = 0.034\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_exp = 5_000\n",
    "\n",
    "for N in [15_000, 5_000, 1_000]:\n",
    "    print(f\"{N} samples\")\n",
    "\n",
    "    cm_metrics_per_team = np.zeros((4, len(metrics), n_exp))\n",
    "    ncm_metrics_per_team = np.zeros((4, len(metrics), n_exp))\n",
    "\n",
    "    for exp in tqdm(range(n_exp)):\n",
    "        N_sample = get_sample(N)\n",
    "        for idt in range(4):\n",
    "            cm = np.zeros((4,4)).astype('int')\n",
    "            detected = np.round(N_sample*rec_c_per_team[idt]).astype('int')\n",
    "            for c in range(4):\n",
    "                cm[c] = np.round(detected[c]*error_distrib_per_team[idt][c])\n",
    "            ncm = np.array([row/row.sum() for row in cm])\n",
    "            for idm,(key,metric) in enumerate(metrics.items()):\n",
    "                cm_metrics_per_team[idt, idm, exp] = metric(cm)\n",
    "                ncm_metrics_per_team[idt, idm, exp] = metric(ncm)\n",
    "    \n",
    "    cm_args = cm_metrics_per_team.argsort(axis=2)\n",
    "    ncm_args = ncm_metrics_per_team.argsort(axis=2)\n",
    "\n",
    "    pmin = int(n_exp*0.975)\n",
    "    pmax = int(n_exp*0.025)\n",
    "\n",
    "    for idm, metric in enumerate(metrics):\n",
    "        max_cm = 0\n",
    "        max_ncm = 0\n",
    "        for idt in range(4):\n",
    "            sorted_team_cm = cm_metrics_per_team[idt, idm, cm_args[idt, idm]]\n",
    "            team_uncertainty_cm = sorted_team_cm[pmin]-sorted_team_cm[pmax]\n",
    "            max_cm = max(team_uncertainty_cm, max_cm)\n",
    "            sorted_team_ncm = ncm_metrics_per_team[idt, idm, ncm_args[idt, idm]]\n",
    "            team_uncertainty_ncm = sorted_team_ncm[pmin]-sorted_team_ncm[pmax]\n",
    "            max_ncm = max(team_uncertainty_ncm, max_ncm)\n",
    "        print(f\"{metric}: CM = {max_cm:.3f}, NCM = {max_ncm:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
